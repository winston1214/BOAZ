안녕하세요 이번에 TransGAN 논문 발표를 맡게 된 김영민이라고 합니다. 이번에 발표할 논문은 따끈따끈한 최신 논문인데요 CVPR2021에 발표 예정인 Transgan 입니다.

이 논문은 현재 Computer Vision 계의 핫한 이슈를 잘 반영해준 논문이라고 할 수 있습니다. 현재 Computer Vision 분야에선 NLP에서 주로 쓰이는 Transformer를 적용해서 성능을 높이고 있습니다. 또한, MLP만으로도 Computer Vision 분야에서 성공적인 퍼포먼스를 보이고 있습니다. 이러한 현재 동향으로 인해 Convolution 연산을 기반으로한 CNN은 갈 곳을 잃어가고 있습니다. CNN이 점점 외면 받는 이유는 최적화의 어려움이 있고 세부정보가 손실되는 문제점이 있기 때문입니다. 그래서 오늘 발표하는 TransGAN은 이러한 Convolution을 사용하지 않는 트랜드를 잘 따라가는 논문입니다.
현재까지 Classification, Segmentation, Detection 분야에선 Convolution 을 사용하지 않고 Transformer를 사용하여 큰 퍼포먼스를 보였지만 Generation 분야에선 아직까지 Transformer만을 사용해서 좋은 퍼포먼스를 보인 연구는 없었습니다. 그래서 이 논문은 최초로 Convolution을 사용하지 않고 Transformer만 사용한 GAN 입니다.

본 논문의 Contribution은 다음과 같습니다. 첫번째로 앞서 말했듯이 Convolution layer를 전혀 사용하지 않은 최초의 GAN 입니다. 기존엔 encoder block에만 transformer를 사용하였는데 본 논문에선 이와는 전혀 다르고 convolution layer에서 완전히 벗어낫다는 것을 강조하였습니다.
두번째론 기존 선행 논문에는 visual Transformer를 사용하였을 때 모델의 encoder 부분에 convolution layer를 사용해서 training하는데 이 때 memory 문제가 있었습니다. 또한, GAN은 일반적으로 불안정하고 모드 충돌이 쉽게 일어나는 문제점이 있었습니다.이를 극복하기 위해 memory를 절약하는 Transformer와 안정적이게 바꾸는 혁신적 방법을 제안합니다. 마지막으로 STL-10 데이터셋으로 모든 성능 부분에서 1등을 하여 SOTA를 달성합니다.

왜 하필 Transformer를 Computer Vision에 적용했을까라는 것에 기존 선행연구에서 밝혀진 내용을 본 논문에선 다음 2가지로 정리합니다.
첫번째로 표현력이 좋고 인간의 정의하는 편향이 없다는 것입니다. CNN과 비교하였을 때, CNN은 모든 필터에 거쳐 가중치를 공유하기 때문에 공간 불변성뿐만 아니라 feature locality에도 강한 편향을 보입니다. 두번째로 Transformer의 구조는 간단하고 범용적으로 쓰일 수 있습니다. 이러한 Transformer는 CNN의 단점을 보완할 수 있는데 이는 CNN에서 많이 보이는 ad-hoc building block을 제거할 수 있다는 것입니다. 

이제부터 본 논문의 제안 방법에 대해서 말씀드리겠습니다. 본 논문에선 Contribution에서도 이미 언급했듯이 Transformer만을 사용하였습니다. 또한 이 Transformer의 강점을 살리기 위해 최소한의 튜닝만을 하였습니다. 그림에 보이는 것처럼 Token이 들어가면 Normalization layer를 거치게 만들었습니다. 거기에 추가로 ResNet의 아이디어를 활용하여 residual connection을 합니다. 그래서 일단 normalization을 거친 값을 첫번째 블락, multi-head self-attention module을 거치게 합니다. 그리고 residual connection을 해주고 feedfoward-MLP를 거치게 합니다. 이 때 여기선 GELU라는 activation function을 사용하면서 비선형성을 높입니다.

다음은 Memory를 효율적으로 할 수 있는 Generator model 입니다. NLP에선 Transformer 모델을 활용 할 때 각각의 단어를 모두 input으로 넣습니다. 그러나 이 방식을 그래도 차용하여 이미지에도 적용하면 pixel-by-pixel을 모두 input으로 넣어야 되는데 이는 메모리에 큰 무리가 있습니다. 예를 들어 작은 크기의 32 by 32 모델을 넣으면 길이가 1024가 됩니다. 이러면 self-attention에도 큰 비용이 발생합니다. 본 논문에선 이를 해결하기 위해 작은 크기에서 시작하여 점차 upscaling하는 방식을 사용합니다. 따라서 본 논문의 전략은 짧은 길이로 시작하여 점차적으로 input sequence를 늘리고 embedding 차원을 줄이는 것입니다.
그림에 대해 설명드리면 input 값으로 noise를 받아서 MLP 층을 통과시킵니다. 통과하고 나온 결과값으론 H by W by C 의 vector를 출력합니다.그리고 이러한 feature map vector는 learnable한 positional encoding과 결합하게 되고 이는 64의 길이를 가진 C 차원의 토큰으로 처리되게 합니다.
그리고 고해상도의 이미지를 처리하기 위헤 pixelshuffle로 구성된 upscaling모듈을 Transformer stage 뒤에 적용시킵니다. 이 과정을 좀 더 자세히 설명하자면 1차원인 sequence를 2차원으로 바꿔준 뒤 h,w에 각각 2배를 시킵니다. 이 때 C는 4분에 1배를 하여 dimension을 줄여줍니다. 그리고 다시 1차원으로 바꿔줍니다. 이러한 과정을 target resolution인 H_t, W_t가 될 때까지 반복을 합니다. 그리고 H_t W_t가 되고 난 후 C를 3으로 바꿔줍니다. 이러한 과정은 메모리에 무리가 가지 않은 이미지 생성 방식으로 계산의 효율성을 갖게 만듭니다. 간단한 아이디어로 Generator의 메모리 효율성을 이뤄냈습니다.

다음으로 Discriminator입니다. 이번 장에서 소개하는 Discriminaotr는 완전 베이스 모델이고 최종적인 Discriminator가 아니라는 점을 먼저 알려드리겠습니다. Generator 모델과 달리 Discriminator는 진짜와 가짜냐만 판단하면 됩니다. 상대적으로 모델이 간단합니다. 일단 입력 이미지를 patch-level로 tokenizing 합니다. 예를 들면 32 by 32 이미지를 8 by 8 이미지로 각각을 단어처럼 patch level로 만듭니다. 그리고 이 patch된 이미지를 linear flatten을 통해 임베딩 토큰 1d sequence로 바꿔주고 차원을 동일하게 C로 맞춰줍니다. 그리고 이 flatten 된 결과들을 transformer encoder를 거치면서 cls 토큰을 추가해줍니다. 그리고 이 cls 토큰으로 진짜인지 가짜인지를 분류합니다.

이러한 네트워크 구조를 가진 TransGan의 실험 결과 입니다. 실험은 4개의 모델로 진행되었습니다. 처음으로 2018년 SOTA를 찍은 AutoGAN, 이 AutoGAN은 일반 GAN 모델이라고 생각하시면 됩니다. 그리고 두번째로 Transformer를 적용한 Generator와 AutoGAN의 Discriminator, 세번째는 AutoGAN의 G와 Transformer의 D, 마지막으로 본 논문의 method인 생성자와 판별자에 모두 Transformer를 적용한 것입니다. 평가지표는 Inception score인 IS와 FID 입니다. IS는 높을수록 performance가 좋은 것이고 FID는 낮을수록 좋은 것입니다. 보시는 바와 같이 Generator에 Transformer를 적용한 것이 IS가 가장 좋았습니다. 반면에 Discriminator에 Transformer를 적용한 것은 모두 낮은 성능을 보였습니다.

Transformer 기반의 분류기는 데이터 부족현상이 나타나게 된다고 합니다. 따라서 논문의 저자는 이러한 문제점을 해결하기 위해 데이터를 증강시키는 방법을 고안했습니다. GAN 분야에서 잘 언급이 되진 않았지만 few-shot 방법이 각광을 받고 있습니다. 이는 SOTA를 달성한 GAN 모델들을 통해서 약간의 이미지로 새로운 이미지를 생성하여 이 생성한 이미지로 데이터를 증강시키는 방법입니다. 따라서 본 논문의 저자는 이 방법을 차용해서 데이터를 증강시킨 후에 다시 여러 모델들과 비교를 하였습니다. 표와 같이 WGAN과 AutoGAN, StyleGAN v2와 TransGAN 모델을 비교하였습니다. 데이터 증강에 따라 눈에 띄는 큰 성능 향상을 보인 모델은 TransGAN이었습니다. 따라서 데이터 증강을 한 후 실험을 진행합니다.

논문의 저자는 데이터를 증강해서 성능을 높인 것에 만족하지 않고 SOTA를 달성하려고 또 다른 실험을 계획합니다. NLP에서 Transformer가 multiple pretraining task 방법을 썼을 때 좋은 성능을 보였습니다. 또한, 이런 방식을 사용하여 self-supervised 보조 테스크를 추가하였을 때 좋은 성능을 냈던 기존의 연구가 존재합니다. 그래서 본 논문은 이를 그대로 차용합니다.따라서 기존의 GAN loss에 SuperResolution 보조 테스크를 더하는 방식으로 진행이 됩니다. 생성자에서 stage를 거칠 때 low resolution 이미지를 넣어주고 이를 super resolution 이미지를 얻어냅니다. 그리고 이를 downsampling 하면 이미지 생성이 더 효과적으로 되는 효과를 얻을 수 있습니다. 그리고 이러한 방식을 적용했을 때 성능은 기존의 데이터 증강을 하였을 때보다 더 좋은 성능을 얻었습니다.

그리고 추가로 방식을 하나 더 사용합니다.바로 Local initialization입니다. CNN 기반의 모델은 이미지를 smooth 하게 하는 기능이 있습니다. 하지만 Transformer 모델은 이러한 부분이 부족합니다. 근데 논문의 저자는 실험을 하다가 Transformer 모델이 convolution 구조를 이미지로부터 배우는 경향이 있다는 것을 발견하였습니다. 그래서 convolution layer의 inductive bias와 Transformer의 flexibility를 같이 인코딩 할 수 있는지 실험하였습니다. 이러한 실험은 기존에 convolution layer를 섞어서 low-level의 이미지 구조를 encoding 하였는데 본 논문에선 warm-starting 만으로도 이를 구현하여 비슷한 효과를 내게 하였습니다. 그래서 self-attention을 이용한 locality-Aware Initialization 방식을 소개합니다. 이 방식은 보시는 그림과 같습니다. mask되지 않은 픽셀만 상호작용이 가능하도록 설정하였습니다. 이러한 방식을 stage마다 mask를 확대하여 최종 스테이지에선 receptive field가 global되게 만들었습니다. 하지만 이 방식은 초기에는 도움이 되지만 훈련을 하면 할 수록 성능에 도움이 안된다는 것을 발견하였습니다. 따라서 본 논문에선 이러한 초기화 방식을 규제로써 초기 train에만 적용을 합니다. 이는 초기 위치의 주변 픽셀 부분에 우선 순위를 두게하고 그 다음에 점점 광범위하게 이미지 생성을 학습할 수 있도록 강제하게 합니다.

결과는 다음과 같이 성공적으로 나왔습니다. 앞서 설명드린 MT-CT 방법과 Local initialization 방법을 모두 적용한 것이 가장 성능이 좋게 나왔습니다.

본 논문의 저자는 이에 그치지 않고 해상도가 큰 모델에도 적용할 수 있다는 것을 보였습니다. 모델을 4가지로 나눠서 실험 결과를 보였습니다. s는 384 384, m은 512 512, L은 768 768, XL은 1024 1024 사이즈 입니다. 그리고 이 결과 고해상도 이미지일수록 퍼포먼스가 좋은 것을 볼 수 있었습니다. 특히 1024 1024모델은 단순히 encoder 블락을 늘리는 것만으로 좋은 성능을 보였습니다.

다른 GAN 모델과 비교했을 때 CIFAR-10 에선 FID 부분에선 2등을 하였습니다. 1등은 역시 StyleGAN v2 였습니다.그리고 STL-10 부분에선 SOTA를 달성하였습니다.

감사합니다.

