안녕하세요 김영민,김지수,이다인으로 구성된 Image Generator조입니다. 오늘 발표는 저와 이다인님 둘이서 진행하도록 하겠습니다. 저희가 오늘 발표할 논문은 Deep Residual Learning for Image regcognition 입니다. 소위 ResNet으로 알려진 논문입니다.

발표는 Introduction, Deep Residual Learning, Experiments 부분으로 나눠지고 제가 Introduction과 Deep Residual Learning 파트를 발표하고 Experiments 부분은 이다인님께서 발표를 진행하겠습니다.

먼저 Introduction 부분입니다.
기존의 CNN 모델은 레이어가 깊어질수록 filter의 개수가 많아지고 너비와 높이는 줄어듭니다. 따라서 서로 다른 특징 정보를 추출하는 activation map은 네트워크가 깊어질수록 풍부한 feature들을 추출하고 이에 따라 더 높은 성능을 띈다는 것이 일반적인 양상이었습니다.
사실 과거에는 Overfitting 문제로 깊이가 깊은 모델이 좋은 퍼포먼스를 보여주지 못하였는데 weight 초기화 방법과 효율적인 normalization 방법의 등장으로 Overfitting 문제를 어느정도 해결하여 좋은 퍼포먼스를 보여주게 되었습니다.
그러나 ResNet은 정말 많은 층으로 구성되어 있는 네트워크가 항상 좋은 성능을 낼까? 라는 의문점을 가짐으로써 시작합니다. 이러한 의문점을 갖고 실험을 진행하는데 layer가 26개인 plain network와 layer가 56개인 plain network의 에러율이 얼마나 나오는지 측정합니다.
실험 결과 다음의 그림과 같습니다. Train 과 Test의 에러율이 모두 56layer를 가진 network가 높았습니다. 이러한 문제를 Degadation라고 하는데 Gradient vanishing 이나 exploding 으로 인해 어느 정도 선에 도달하게 되면 퍼포먼스가 낮아지는 것을 말합니다.

다시 한 번 정확히 개념을 짚고 넘어가겠습니다. 간단하게 말하면 Overfitting은 train 성능은 좋은데 test 성능은 낮은 것을 말하고 Degradation은 train과 test 성능이 모두 좋지 않은 것을 말합니다.
이러한 Degradation 문제를 해결하기 위해 본 논문에선 Residual Learning을 제안합니다.
이 그림은 ResNet의 핵심인 Residual Block 입니다. Residual Block은 단순한 값 x를 활성화 함수를 통과한 복잡한 값에 더해주는 형식입니다. 이 Residual block은 추후에 다시 설명하겠습니다.
그래서 본 논문은 Original mapping 즉, 원래의 CNN 구조를 Optimize 하기엔 어렵고 논문에서 제안하는 Residaul Block은 Optimize하기에 쉽다고 주장합니다.

이러한 이유는 Plain Layer 즉 앞서 말했던 Original mapping과 Residual layer의 수식으로 설명 가능합니다. 먼저 H(x)는 기존의 Original mapping 값을 말하고 F(x)는 잔차, x는 input 값입니다. 
Plain layer를 먼저 살펴보면 기존의 cnn 처럼 입력 x에 weight layer 즉 convolution 연산을 하고 특징을 추출합니다. ReLU와 같은 activation 함수를 통해서 전체 네트워크가 nonlinear하게 동작할 수 있도록 합니다.
반면에 Residual block은 단순한 특성 x를 변형 없이 그대로 가져오고 거기에 복잡한 특성 즉 추가적으로 남아있는 잔여 정보 F를 더해주는 것입니다.
네트워크의 구조와 식을 통해 알 수 있듯이 H(x)를 x로 만드는 훈련보다 잔차인 F(x)를 0으로 만드는 훈련이 더 간단하고 쉽기 때문에 Residual block 방법을 이용합니다.
그리고 Residual block을 이용해 feedbackfoward network에 적용한 것을 Identity shortcut이라고 합니다.

Shortcut connection은 한개 이상의 layer를 skip하는 것입니다. shortcut connection을 하기 위해선 성립되어야하는 조건이 있는데 input layer의 dimension과 output layer의 demension이 같아야 한다는 것입니다. 이러한 조건을 맞춰주기 위해서 두가지 종류의 shortcut connection이 존재합니다.
첫번째로 intput demension과 output demension이 동일할 때의 shortcut connection을 Identity shortcut이라고 합니다. Identity shortcut의 식은 이 식을 간단히 한건데 여기서 sigma는 ReLu 함수이고 x가 이 ReLU 함수를 통과하고 bias를 생략한 식입니다. 이 식에서 x는 input vector,y는 output vector, W는 weight layer를 의미합니다.
두번째로 input demension과 output demension이 다른 경우입니다. 전체적인 식은 Identity shortcut과 동일하나 W_s와 x를 내적해준 값을 더해주는 것에서 차이가 있습니다. W_s 는 단순히 input demension과 output demension을 동일하게 만들기 위한 square matrix입니다. 
resnet의 아키텍쳐는 다인님이 설명해주실 것입니다.

[1]
이제부터는 앞에서 살펴보았던 ResNet 모델과 다양한 형태의 plain network들에 대해 비교를 진행하는 과정을 통해 ResNet이 기존의 네트워크들에 비해 어떤 성능을 보이는지 살펴보겠습니다. 먼저 ImageNet dataset을 가지고 실험을 진행하기 위해서 두가지 모델 즉 plain 모델, Residual 모델을 각각 정의를 하는데요,
여기 보이는 모델이 바로 34- layer의 plain 모델입니다. 이 모델에 구성에 대해 간단히 살펴보면, 먼저 이 네트워크는 VGG 네트워크의 뼈대를 그대로 가져와 이용한 모델입니다.
따라서 convolution layer는 대개 3 x 3의 필터를 가지고 있고, 두가지의 간단한 규칙에 따라 디자인이 됩니다, 먼저 첫번째로 동일한 output feature map size에 대해서 layer는 동일한 수의 필터를 갖게되고 , 또 두번째로 feature map size가 절반인 경우에는 filter의 개수를 2배로 늘려서 각 layer마다의 time-complexity를 보존하는 형태로 구성됩니다. 
또한 별도의 pooling layer는 사용하지 않고, convolution layer에서 stride 값을 2로 설정해서 down sampling을 진행하고, 네트워크의 마지막에는 총 1000개의 클래스로 분류하기 위해서 average pooling을 사용하고 있습니다.
이렇게 구성된 모델은 기존의 VGG-19모델에 비해서 적은 수의 파라미터를 사용하고, complexity가 낮다는 장점을 가지고 있습니다.
[2]
다음으로 살펴볼 모델은 방금 살펴본 Plain network를 기반으로 해서 residual network를 구성한 모델입니다. 앞의 plain model에서 convolution filter를 두개씩 묶어서 매번 residual function 형태로 학습을 진행하였으며, 위쪽의 화살표에서 점선으로 표시된 부분은 입력단과 출력단의
 dimension이 일치하지 않아서 dimension을 맞춰줄 short connection 테크닉이 가미된 것을 의미합니다. 이렇게 구성된 34-layer의 ResNet은 마찬가지로 기존의 VGG 네트워크와 비교했을 때 딥러닝 모델에서 계산복잡도를 나타내는 척도인 FLOPs(플롭스)가 감소했다는 특징이 있습니다. 

[3]
이제 이렇게 정의한 두가지 모델을 이용해 ImageNet dataset에 대한 실험을 진행하게 됩니다 
먼저 ImageNet dataset은 모델의 학습 및 테스트에 사용되는 training/ validation/ test 이미지의 개수가 보는 것과 같이 각각 128만장, 5만장, 10만장이고 총 1000개의 클래스로 분류됩니다.
이러한 dataset을 가지고 논문에서는 총 3가지 포인트에 대해서 실험을 진행하게 됩니다.
먼저 첫번째는 앞에서 정의했던 plain network와 ResNet의 성능을 비교하고, 두번째로는 두가지 shortcut인 Identity shortcut과 projection shortcut이 ResNet의 performance에 어떻게 영향을 미치는지 마지막으로 ResNet의 layer가 늘어남에 따라
 degradation이 발생하지 않는지 확인하는 실험을 진행합니다.
[4]
첫번째로 plain network 와 ResNet network의 18 layer, 34 layer 각각에 따른 training error와 validation error를 비교하는 모습입니다.
먼저 plain network를 살펴보겠습니다. 34-layer를 의미하는 빨간색 그래프를 살펴보면 plain network에서는 더 깊은 layer를 쌓았을 때 오히려 더 얕은 네트워크보다 에러율이 상승한 것을 확인할 수 있습니다. 논문에서는 이와 같은 현상이 발생하는 이유가
 vanishing gradient 때문은 아니며, exponentially low convergence rates 때문이며 이 부분에 대해서는 추가적인 연구를 진행할 것이라고 언급하고 있습니다.
반면에 ResNet의 경우에는 보는 것과 같이 layer가 깊어질수록 성능의 정확도가 더욱 상승하는 것을 볼 수 있습니다. 특히 34-layer가 18-layer보다 train, validation error가 모두 낮다는 것을 통해 Degradation problem을 잘 해결했다는 것또한 확인할 수 있습니다. 
추가적으로 18-layer plain과 18-layer ResNet 간에는 서로 유사한 성능을 보였지만, 18-layer ResNet이 더욱 빠르게 수렴하는 것을 볼 수 있는데, 이는 ResNet이 초기 단계에서 더욱 빠르게 수렴할 수 있도록 만들어주어 optimization 자체를 더 쉽게 만들어줄 수 있다는 것을 의미합니다.
[5] 
다음으로 두가지의 shortcut이 resnet에 어떤 영향을 미치는지 확인하는 실험입니다. 이때  ResNet을 A,B,C 총 3가지 옵션으로 나누어 비교하는 실험을 진행하게 되는데 A는 zero padding을 사용해 dimension을 늘려주고 identity mapping을 사용하는 것이고, B는 dimension이
 증가할때만 projection을, C는 모든 연산에 대해 projection을 수행하는 것입니다.
이때 왼쪽의 표의 error rate를 보면, ResNet C가 가장 좋은 퍼포먼스를 보여주는데, 하지만 논문에서는 그 차이가 미세하고 C를 사용했을 경우 memory/time complexity가 늘어나기 때문에 효율적이지 않다고 언급하며,
 projection shortcut이 성능 저하 문제를 해결하는데 필수적이지 않다는 것을 나타내고 있습니다. 
[6] 
ImageNet을 이용한 마지막 실험은 ResNet의 depth가 깊어짐에 따라 degradation문제가 발생하는가에 대한 실험입니다. 여기서는 34,50,101 그리고 152 layer의 모델을 비교하게 되는데,
이때 34layer에서는 왼쪽에 보이는 기본 building block을 사용하지만 나머지 더 깊은 모델들을 구성할 때는 training하는데 걸리는 시간을 고려해 기존 building block을 bottleneck 디자인으로 변경하여 layer를 구성합니다. 이러한 bottleneck 디자인을 간단히 살펴보면,
 3개의 layer는 각각 순서대로 1x1, 3x3, 1x1 conv layer이며, 1x1 conv layer는 dimension을 줄이거나 늘리는 용도로 사용하고 3x3 layer의 input/output의 dimension을 줄인 bottleneck으로 두게 됩니다.
여기서 parameter-free인 ideneity shortcut은 이 architecture에서 특히 중요하다는 것 또한 주목해볼 만한 점인데, 만약 오른쪽 bottleneck 구조에서 identity shortcut이 projection으로 대체된다면, shortcut이 두 개의 high-dimensional
 출력과 연결되므로 time complexity와 model size가 두 배로 늘어나게 됩니다. 따라서 identity shortcut은 이 bottleneck design을 보다 효율적인 모델로 만들어준다는 사실 또한 알 수 있습니다. 
이렇게 모델을 구성한 뒤 오른쪽 표의 최종 결과를 살펴보면, 152 layer의 모델이 가장 좋은 성능을 보이고 있다는 것을 확인할 수있고, 따라서 ResNet은 layer가 깊어짐에 따라 degradation문제가 발생하지 않는다는 것을 알 수있습니다. 
[7]
다음으로는 ImageNet이 아닌 CIFAR-10 dataset을 이용해 실험을 진행합니다. 
CIFAR-10 데이터는 일단 입력 이미지 크기가 32x32로 imagenet보다 훨씬 작기 때문에, 
따라서 이에 맞춰 더욱더 파라미터 수를 줄여서 별도의 resnet을 고안해 사용하게 됩니다.
이번 CIFAR-10을 이용한 실험은 앞의 ImageNet보다 훨씬 더 깊은 네트워크에서 ResNet이 error rate를 줄여줄 수 있는가를 확인하기 위한 실험입니다.
[8]
여기보이는 그래프들은 왼쪽부터 차례대로 plain network, resnet, 그리고 가장 오른쪽은 깊이가 매우 깊은 resnet의 training error와 testing error를 나타낸 그래프입니다.
plain network의 경우 ImageNet에서와 마찬가지로 depth가 높아질 수록 성능이 하락하는 현상이 보였는데, 이는 optimization difficulty가 특정 dataset에만 국한된 것이 아닌 본질적인 문제임을 시사합니다.
 ResNet의 20, 32, 44, 56, 110 layer를 비교한 가운데 그래프의 경우는 layer가 깊어질수록 좋은 성능을 나타내는 것을 확인할 수 있습니다. 하지만 가장 우측의 110 layer와 1202 layer를 비교했을 때는 1202 layer의 error가 더 높은 것을 확인할 수 있는데, 
논문에서 이는 소규모 dataset에 비해 모델의 불필요하게 깊은 layer때문에 overfitting이 일어난 것이라고 언급하며, 하지만 1202 layer모델의 경우 dropout이나 maxout 과 같은 강력한 regularization 기법을 사용하지 않았기 때문에 이와 같은 기법을 결합한다면
 더욱 향상된 결과를 기대할 수 있다고 덧붙이고 있습니다.
[9]
마지막으로 object detection과 segmentation task에서 ResNet의 성능을 살펴본 실험입니다. 
마찬가지로 ResNet을 이용한 모델이 훨씬 더 좋은 성과를 거두는 것을 통해 ResNet의 성능을 다시 한번 입증해주고 있습니다.

감사함니다^_^